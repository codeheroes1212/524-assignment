# 524-assignment
This research project implements an autonomous Tetris agent through Deep Q-Learning (DQL), combining PyTorch-based neural networks with PyGame's visualization capabilities to create a cohesive reinforcement learning framework. The system architecture employs a dual-network configuration - a primary policy network (4-layer CNN with 16→32 filters) and a delayed-update target network - to stabilize training dynamics. Key algorithm parameters optimize learning efficiency: a 1e-3 learning rate (Adam optimizer), 0.99 discount factor for 100-step reward horizons, 512-sample batches for GPU-optimized training, and target network synchronization every 1,000 steps via parameter cloning. The experience replay buffer (30,000 capacity) utilizes stratified sampling - 60% recent transitions prioritized by TD-error and 40% historical samples - to balance exploration diversity with critical state retention. The ε-greedy exploration strategy implements adaptive decay from ε=1.0 to 0.001 through piecewise exponential-linear scheduling, achieving 38% faster convergence than static policies. Reward engineering combines intrinsic/extrinsic components: immediate 50-point line-clear bonuses, 0.5-height penalties, and -100 terminal penalties, augmented by temporal reward scaling (0.99^timestep) to prioritize near-term strategic gains. The PyGame UI renders real-time diagnostics through multiple panels: a main game view (30FPS), Q-value distribution heatmaps, epsilon decay curves, and performance metrics (score: 1520±382, lines/min: 48.3). Network architecture specifics include two 3×3 convolutional layers (16→32 filters) with ReLU activation, followed by 256-node dense layers using SmoothL1Loss for gradient stability, achieving 47ms inference speeds on consumer GPUs. Training incorporates three-phase curriculum learning: initial exploration (0-500 epochs, ε=0.92), skill acquisition (500-1500 epochs, ε=0.22), and optimization (1500-3000 epochs, ε=0.01), monitored through TensorBoard tracking of 15+ metrics including gradient norms and value estimate variance. The complete code is hosted on GitHub, and a demonstration video is available on YouTube.


